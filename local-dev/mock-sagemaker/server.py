"""
Mock SageMaker Endpoint Server

This simulates a SageMaker endpoint for local development.
It returns predefined responses based on the input prompt.

For AI/ML Scientists:
- This is NOT a real LLM - it's a lookup table
- Matches patterns in prompts and returns canned responses
- Useful for testing agent logic without GPU costs
- Response time is instant (vs. 2-5 seconds for real LLM)
"""

from flask import Flask, request, jsonify
import json
import os
import logging

app = Flask(__name__)
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

# Load response templates
RESPONSES_DIR = 'responses'


def load_responses():
    """Load all response templates from responses/ directory"""
    responses = {}
    if os.path.exists(RESPONSES_DIR):
        for filename in os.listdir(RESPONSES_DIR):
            if filename.endswith('.json'):
                key = filename.replace('.json', '')
                with open(os.path.join(RESPONSES_DIR, filename)) as f:
                    responses[key] = json.load(f)
    return responses


# Response templates (fallback if files don't exist)
DEFAULT_RESPONSES = {
    'tool_call': {
        'generated_text': '<tool>tavily_search_results_json</tool><tool_input>latest UK storm</tool_input>'
    },
    'final_answer': {
        'generated_text': '<final_answer>This is a mock answer from the fake LLM. In production, this would be generated by Mistral 7B on SageMaker.</final_answer>'
    },
    'math': {
        'generated_text': '<final_answer>4</final_answer>'
    },
    'capital': {
        'generated_text': '<final_answer>The capital of France is Paris.</final_answer>'
    }
}


def select_response(prompt):
    """
    Select appropriate response based on prompt content

    This is a simple keyword matcher. A real LLM would use:
    - Tokenization
    - Neural network forward pass
    - Beam search / sampling
    - Detokenization

    But for local testing, pattern matching is sufficient.
    """
    prompt_lower = prompt.lower()

    # Check for tool call keywords
    if any(keyword in prompt_lower for keyword in ['latest', 'current', 'recent', 'today']):
        logger.info("Detected: Question requiring search → returning tool call")
        return DEFAULT_RESPONSES['tool_call']

    # Check for math
    if any(op in prompt for op in ['+', '-', '*', '/', '2+2', '2 + 2']):
        logger.info("Detected: Math question → returning 4")
        return DEFAULT_RESPONSES['math']

    # Check for capitals
    if 'capital' in prompt_lower and 'france' in prompt_lower:
        logger.info("Detected: Capital of France → returning Paris")
        return DEFAULT_RESPONSES['capital']

    # Check if this is the second call (has observation from tool)
    if '<observation>' in prompt or 'tavily_search' in prompt:
        logger.info("Detected: Second agent call (after tool) → returning final answer")
        return {
            'generated_text': '<final_answer>Based on the search results, Storm Henk was the latest storm to hit the UK, causing damage in south-west England.</final_answer>'
        }

    # Default response
    logger.info("No pattern matched → returning default")
    return DEFAULT_RESPONSES['final_answer']


@app.route('/ping', methods=['GET'])
def ping():
    """Health check endpoint"""
    return jsonify({'status': 'healthy'}), 200


@app.route('/invocations', methods=['POST'])
def invocations():
    """
    Main inference endpoint (mimics SageMaker invoke_endpoint)

    Expected payload (from ContentHandler):
    {
        "inputs": "<s>[INST] Your question [/INST]",
        "parameters": {
            "max_new_tokens": 500,
            "temperature": 0.001
        }
    }

    Response format (SageMaker HuggingFace TGI):
    [
        {
            "generated_text": "The answer is...",
            "details": {...}  # Optional
        }
    ]
    """
    try:
        payload = request.get_json()

        if not payload or 'inputs' not in payload:
            return jsonify({'error': 'Missing inputs field'}), 400

        prompt = payload['inputs']
        parameters = payload.get('parameters', {})

        logger.info(f"Received request - prompt length: {len(prompt)}")
        logger.debug(f"Prompt: {prompt[:200]}...")
        logger.debug(f"Parameters: {parameters}")

        # Select response based on prompt
        response_data = select_response(prompt)

        # Format as SageMaker response (array with one element)
        response = [response_data]

        logger.info(f"Returning response: {response_data['generated_text'][:100]}...")

        return jsonify(response), 200

    except Exception as e:
        logger.error(f"Error processing request: {e}")
        return jsonify({'error': str(e)}), 500


@app.route('/', methods=['GET'])
def root():
    """Info endpoint"""
    return jsonify({
        'service': 'Mock SageMaker Endpoint',
        'model': 'mistral-7b-mock',
        'note': 'This is a mock server for local development. It does not run a real LLM.',
        'endpoints': {
            '/ping': 'Health check',
            '/invocations': 'Inference endpoint (POST)'
        }
    })


if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8080))
    app.run(host='0.0.0.0', port=port, debug=True)
